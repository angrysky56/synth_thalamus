{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Thalamus for bAbI QA Tasks\n",
    "\n",
    "This notebook demonstrates the use of the Synthetic Thalamus for reasoning tasks using the bAbI dataset, which contains 20 different types of text-based question answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import random\n",
    "\n",
    "# Add the parent directory to the path\n",
    "sys.path.append('..')\n",
    "from core.thalamus import SyntheticThalamus\n",
    "from core.adapters import TextAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare the bAbI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download the bAbI dataset\n",
    "def download_babi_data():\n",
    "    url = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'\n",
    "    filename = 'babi_tasks_1-20_v1-2.tar.gz'\n",
    "    data_dir = 'data/babi'\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir, filename)):\n",
    "        print(f\"Downloading bAbI dataset...\")\n",
    "        urllib.request.urlretrieve(url, os.path.join(data_dir, filename))\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir, 'tasks_1-20_v1-2')):\n",
    "        print(f\"Extracting bAbI dataset...\")\n",
    "        with tarfile.open(os.path.join(data_dir, filename), 'r:gz') as tar:\n",
    "            tar.extractall(data_dir)\n",
    "            \n",
    "    print(\"Dataset ready.\")\n",
    "    return os.path.join(data_dir, 'tasks_1-20_v1-2', 'en-10k')\n",
    "\n",
    "babi_dir = download_babi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the bAbI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def parse_babi_task(task_id):\n",
    "    \"\"\"Parse a bAbI task file into stories, questions, and answers.\"\"\"\n",
    "    task_file = os.path.join(babi_dir, f\"qa{task_id}_train.txt\")\n",
    "    \n",
    "    stories, questions, answers = [], [], []\n",
    "    story = []\n",
    "    \n",
    "    with open(task_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            nid, text = line.split(' ', 1)\n",
    "            nid = int(nid)\n",
    "            \n",
    "            if nid == 1:\n",
    "                story = []\n",
    "                \n",
    "            if '\\t' in text:  # This line contains a question\n",
    "                q, a = text.split('\\t')\n",
    "                q = q.strip()\n",
    "                a = a.strip()\n",
    "                \n",
    "                # Store the story, question, and answer\n",
    "                stories.append(' '.join(story))\n",
    "                questions.append(q)\n",
    "                answers.append(a)\n",
    "            else:\n",
    "                story.append(text)\n",
    "                \n",
    "    return stories, questions, answers\n",
    "\n",
    "# Parse bAbI task 1 (single supporting fact)\n",
    "stories, questions, answers = parse_babi_task(1)\n",
    "\n",
    "print(f\"Number of examples: {len(stories)}\")\n",
    "print(\"\\nExample:\")\n",
    "print(f\"Story: {stories[0]}\")\n",
    "print(f\"Question: {questions[0]}\")\n",
    "print(f\"Answer: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenization and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_vocabulary(stories, questions, answers):\n",
    "    \"\"\"Create a vocabulary from all words in the dataset.\"\"\"\n",
    "    vocab = set()\n",
    "    for story in stories:\n",
    "        for word in story.split():\n",
    "            vocab.add(word.lower())\n",
    "    for question in questions:\n",
    "        for word in question.split():\n",
    "            vocab.add(word.lower())\n",
    "    for answer in answers:\n",
    "        vocab.add(answer.lower())\n",
    "    \n",
    "    # Create word-to-index mapping\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word in vocab:\n",
    "        word_to_idx[word] = len(word_to_idx)\n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "def tokenize(text, word_to_idx, max_len=None):\n",
    "    \"\"\"Convert text to token indices.\"\"\"\n",
    "    tokens = [word_to_idx.get(word.lower(), word_to_idx['<UNK>']) for word in text.split()]\n",
    "    if max_len is not None:\n",
    "        if len(tokens) < max_len:\n",
    "            tokens += [word_to_idx['<PAD>']] * (max_len - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "# Create vocabulary\n",
    "word_to_idx = create_vocabulary(stories, questions, answers)\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create answer-to-index mapping\n",
    "answer_to_idx = {}\n",
    "for answer in set(answers):\n",
    "    answer_to_idx[answer.lower()] = len(answer_to_idx)\n",
    "num_answers = len(answer_to_idx)\n",
    "print(f\"Number of unique answers: {num_answers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bAbI Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BabiDataset(Dataset):\n",
    "    def __init__(self, stories, questions, answers, word_to_idx, answer_to_idx, max_story_len=100, max_question_len=20):\n",
    "        self.stories = stories\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.answer_to_idx = answer_to_idx\n",
    "        self.max_story_len = max_story_len\n",
    "        self.max_question_len = max_question_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.stories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        story = tokenize(self.stories[idx], self.word_to_idx, self.max_story_len)\n",
    "        question = tokenize(self.questions[idx], self.word_to_idx, self.max_question_len)\n",
    "        answer = self.answer_to_idx[self.answers[idx].lower()]\n",
    "        \n",
    "        return {\n",
    "            'story': torch.tensor(story, dtype=torch.long),\n",
    "            'question': torch.tensor(question, dtype=torch.long),\n",
    "            'answer': torch.tensor(answer, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BabiDataset(stories, questions, answers, word_to_idx, answer_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Thalamus-based QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ThalamusQAModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, num_answers, d_model=128, task_id=0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Embedding layers for story and question\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Story encoder\n",
    "        self.story_encoder = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Question encoder\n",
    "        self.question_encoder = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Synthetic thalamus for token gating\n",
    "        self.thalamus = SyntheticThalamus(\n",
    "            d_model=d_model * 2,  # Bidirectional LSTM output\n",
    "            n_heads=4,\n",
    "            k=16,\n",
    "            phase_dim=16,\n",
    "            task_dim=64,\n",
    "            num_tasks=20  # 20 different bAbI tasks\n",
    "        )\n",
    "        \n",
    "        # Workspace transformer for reasoning\n",
    "        self.workspace = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model * 2 + 16,  # Bidirectional LSTM + phase dim\n",
    "                nhead=4,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Answer predictor\n",
    "        self.answer_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model * 2 + 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_answers)\n",
    "        )\n",
    "        \n",
    "        self.task_id = task_id\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, story, question, task_id=None):\n",
    "        if task_id is None:\n",
    "            task_id = torch.tensor([self.task_id] * story.size(0), dtype=torch.long, device=story.device)\n",
    "        \n",
    "        # Embed story and question\n",
    "        story_emb = self.embedding(story)     # [B, story_len, d_model]\n",
    "        question_emb = self.embedding(question)  # [B, question_len, d_model]\n",
    "        \n",
    "        # Encode story and question\n",
    "        story_encoded, _ = self.story_encoder(story_emb)  # [B, story_len, 2*d_model]\n",
    "        question_encoded, _ = self.question_encoder(question_emb)  # [B, question_len, 2*d_model]\n",
    "        \n",
    "        # Use question encoding as context for the thalamus\n",
    "        # Average the question encoding to get a single vector\n",
    "        question_context = question_encoded.mean(dim=1, keepdim=True)  # [B, 1, 2*d_model]\n",
    "        \n",
    "        # Apply synthetic thalamus to story tokens with question as context\n",
    "        gated_tokens = self.thalamus(story_encoded, task_id, question_context)  # [B, k, 2*d_model+phase_dim]\n",
    "        \n",
    "        # Process gated tokens with transformer workspace\n",
    "        workspace_output = self.workspace(gated_tokens)  # [B, k, 2*d_model+phase_dim]\n",
    "        \n",
    "        # Pooling (mean across tokens)\n",
    "        pooled = workspace_output.mean(dim=1)  # [B, 2*d_model+phase_dim]\n",
    "        \n",
    "        # Predict answer\n",
    "        logits = self.answer_predictor(pooled)  # [B, num_answers]\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        story = batch['story']\n",
    "        question = batch['question']\n",
    "        answer = batch['answer']\n",
    "        \n",
    "        logits = self(story, question)\n",
    "        loss = self.loss_fn(logits, answer)\n",
    "        \n",
    "        # Log accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == answer).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        story = batch['story']\n",
    "        question = batch['question']\n",
    "        answer = batch['answer']\n",
    "        \n",
    "        logits = self(story, question)\n",
    "        loss = self.loss_fn(logits, answer)\n",
    "        \n",
    "        # Log accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == answer).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Split and Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create train/validation split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize model\n",
    "model = ThalamusQAModel(vocab_size=vocab_size, num_answers=num_answers, task_id=0)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(monitor='val_acc', mode='max'),\n",
    "        pl.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Thalamus Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to visualize which story tokens were gated by the thalamus\n",
    "def visualize_token_gating(model, story, question, answer, word_to_idx, idx_to_word=None):\n",
    "    if idx_to_word is None:\n",
    "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    story_tokens = tokenize(story, word_to_idx, max_len=100)\n",
    "    question_tokens = tokenize(question, word_to_idx, max_len=20)\n",
    "    \n",
    "    # Convert to tensors and add batch dimension\n",
    "    story_tensor = torch.tensor(story_tokens, dtype=torch.long).unsqueeze(0)\n",
    "    question_tensor = torch.tensor(question_tokens, dtype=torch.long).unsqueeze(0)\n",
    "    task_id = torch.tensor([0], dtype=torch.long)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Embed story and question\n",
    "        story_emb = model.embedding(story_tensor)\n",
    "        question_emb = model.embedding(question_tensor)\n",
    "        \n",
    "        # Encode story and question\n",
    "        story_encoded, _ = model.story_encoder(story_emb)\n",
    "        question_encoded, _ = model.question_encoder(question_emb)\n",
    "        \n",
    "        # Use question encoding as context for the thalamus\n",
    "        question_context = question_encoded.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Extract salience scores from the thalamus (without gating)\n",
    "        B, N, D = story_encoded.size()\n",
    "        task_embedding = model.thalamus.task_embed(task_id)\n",
    "        task_embedding = model.thalamus.task_proj(task_embedding)\n",
    "        task_embedding = task_embedding.unsqueeze(1).expand(-1, N, -1)\n",
    "        x_combined = story_encoded + task_embedding\n",
    "        x_attn, _ = model.thalamus.scorer(x_combined, question_context, question_context)\n",
    "        scores = x_attn.norm(dim=-1)  # [B, N]\n",
    "        \n",
    "        # For visualization, get logits and prediction\n",
    "        logits = model(story_tensor, question_tensor)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Get topk indices\n",
    "        _, topk_indices = scores[0].topk(model.thalamus.k)\n",
    "        topk_indices = topk_indices.cpu().numpy()\n",
    "    \n",
    "    # Map tokens back to words\n",
    "    story_words = [idx_to_word[idx] for idx in story_tokens if idx != word_to_idx['<PAD>']]\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(range(len(story_words)), scores[0, :len(story_words)].cpu().numpy())\n",
    "    \n",
    "    # Highlight top-k tokens\n",
    "    for idx in topk_indices:\n",
    "        if idx < len(story_words):\n",
    "            plt.bar(idx, scores[0, idx].cpu().numpy(), color='red')\n",
    "    \n",
    "    plt.xticks(range(len(story_words)), story_words, rotation=45, ha='right')\n",
    "    plt.title(f\"Question: {question} | Predicted Answer: {pred} | True Answer: {answer}\")\n",
    "    plt.xlabel('Story Tokens')\n",
    "    plt.ylabel('Salience Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the gated tokens\n",
    "    print(\"Gated tokens:\")\n",
    "    for idx in topk_indices:\n",
    "        if idx < len(story_words):\n",
    "            print(f\"- {story_words[idx]} (score: {scores[0, idx]:.4f})\")\n",
    "\n",
    "# Create inverse mapping for visualization\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Get a sample for visualization\n",
    "sample_idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[sample_idx]\n",
    "story_tokens = sample['story'].tolist()\n",
    "question_tokens = sample['question'].tolist()\n",
    "answer_idx = sample['answer'].item()\n",
    "\n",
    "# Convert back to words for display\n",
    "story_words = ' '.join([idx_to_word[idx] for idx in story_tokens if idx != word_to_idx['<PAD>']])\n",
    "question_words = ' '.join([idx_to_word[idx] for idx in question_tokens if idx != word_to_idx['<PAD>']])\n",
    "answer_word = list(answer_to_idx.keys())[list(answer_to_idx.values()).index(answer_idx)]\n",
    "\n",
    "print(\"Sample Story:\")\n",
    "print(story_words)\n",
    "print(\"\\nQuestion:\")\n",
    "print(question_words)\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer_word)\n",
    "\n",
    "# Visualize the token gating\n",
    "visualize_token_gating(model, story_words, question_words, answer_word, word_to_idx, idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Task IDs\n",
    "\n",
    "The synthetic thalamus can adapt its gating behavior based on the task ID. Let's investigate how different task IDs affect the model's performance and gating pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_with_different_tasks(model, dataset, num_tasks=5):\n",
    "    \"\"\"Evaluate the model with different task IDs and visualize differences.\"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    task_accuracies = []\n",
    "    \n",
    "    for task_id in range(num_tasks):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                story = batch['story']\n",
    "                question = batch['question']\n",
    "                answer = batch['answer']\n",
    "                \n",
    "                # Set the task ID explicitly\n",
    "                task_ids = torch.full((story.size(0),), task_id, dtype=torch.long)\n",
    "                \n",
    "                logits = model(story, question, task_ids)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                total += answer.size(0)\n",
    "                correct += (preds == answer).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        task_accuracies.append(accuracy)\n",
    "        print(f\"Task ID {task_id}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(num_tasks), task_accuracies)\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance with Different Task IDs')\n",
    "    plt.xticks(range(num_tasks))\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the gating pattern for the same example with different task IDs\n",
    "    sample_idx = random.randint(0, len(dataset) - 1)\n",
    "    sample = dataset[sample_idx]\n",
    "    \n",
    "    story_tokens = sample['story'].tolist()\n",
    "    question_tokens = sample['question'].tolist()\n",
    "    answer_idx = sample['answer'].item()\n",
    "    \n",
    "    # Convert back to words for display\n",
    "    story_words = ' '.join([idx_to_word[idx] for idx in story_tokens if idx != word_to_idx['<PAD>']])\n",
    "    question_words = ' '.join([idx_to_word[idx] for idx in question_tokens if idx != word_to_idx['<PAD>']])\n",
    "    answer_word = list(answer_to_idx.keys())[list(answer_to_idx.values()).index(answer_idx)]\n",
    "    \n",
    "    print(\"\\nSample Story:\")\n",
    "    print(story_words)\n",
    "    print(\"\\nQuestion:\")\n",
    "    print(question_words)\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(answer_word)\n",
    "    \n",
    "    # Compare gating patterns for each task ID\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, task_id in enumerate(range(min(4, num_tasks))):\n",
    "        # Prepare inputs\n",
    "        story_tensor = torch.tensor(story_tokens, dtype=torch.long).unsqueeze(0)\n",
    "        question_tensor = torch.tensor(question_tokens, dtype=torch.long).unsqueeze(0)\n",
    "        task_tensor = torch.tensor([task_id], dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process through the model\n",
    "            story_emb = model.embedding(story_tensor)\n",
    "            question_emb = model.embedding(question_tensor)\n",
    "            \n",
    "            story_encoded, _ = model.story_encoder(story_emb)\n",
    "            question_encoded, _ = model.question_encoder(question_emb)\n",
    "            \n",
    "            question_context = question_encoded.mean(dim=1, keepdim=True)\n",
    "            \n",
    "            # Extract salience scores\n",
    "            B, N, D = story_encoded.size()\n",
    "            task_embedding = model.thalamus.task_embed(task_tensor)\n",
    "            task_embedding = model.thalamus.task_proj(task_embedding)\n",
    "            task_embedding = task_embedding.unsqueeze(1).expand(-1, N, -1)\n",
    "            x_combined = story_encoded + task_embedding\n",
    "            x_attn, _ = model.thalamus.scorer(x_combined, question_context, question_context)\n",
    "            scores = x_attn.norm(dim=-1)[0, :len(story_words)].cpu().numpy()\n",
    "            \n",
    "            # Make prediction\n",
    "            logits = model(story_tensor, question_tensor, task_tensor)\n",
    "            pred = torch.argmax(logits, dim=1).item()\n",
    "            pred_word = list(answer_to_idx.keys())[list(answer_to_idx.values()).index(pred)]\n",
    "            \n",
    "            # Plot\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            plt.bar(range(len(story_words)), scores)\n",
    "            plt.title(f\"Task ID {task_id} | Predicted: {pred_word}\")\n",
    "            plt.xticks(range(len(story_words)), [idx_to_word[idx] for idx in story_tokens[:len(story_words)]], \n",
    "                      rotation=45, ha='right', fontsize=8)\n",
    "            plt.ylabel('Salience Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate with different task IDs\n",
    "evaluate_with_different_tasks(model, val_dataset, num_tasks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_phase_embeddings(model, sample):\n",
    "    \"\"\"Visualize the phase embeddings produced by the thalamus.\"\"\"\n",
    "    story_tokens = sample['story']\n",
    "    question_tokens = sample['question']\n",
    "    answer_idx = sample['answer'].item()\n",
    "    \n",
    "    # Add batch dimension\n",
    "    story_tensor = story_tokens.unsqueeze(0)\n",
    "    question_tensor = question_tokens.unsqueeze(0)\n",
    "    task_id = torch.tensor([0], dtype=torch.long)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the embeddings\n",
    "        story_emb = model.embedding(story_tensor)\n",
    "        question_emb = model.embedding(question_tensor)\n",
    "        \n",
    "        # Encode story and question\n",
    "        story_encoded, _ = model.story_encoder(story_emb)\n",
    "        question_encoded, _ = model.question_encoder(question_emb)\n",
    "        \n",
    "        # Use question encoding as context for the thalamus\n",
    "        question_context = question_encoded.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply thalamus\n",
    "        gated_tokens = model.thalamus(story_encoded, task_id, question_context)\n",
    "        \n",
    "        # Extract the phase embeddings (last phase_dim dimensions)\n",
    "        phase_dim = model.thalamus.phase_dim\n",
    "        phase_embeddings = gated_tokens[0, :, -phase_dim:].cpu().numpy()\n",
    "        \n",
    "        # Get token indices for the top-k gated tokens\n",
    "        B, N, D = story_encoded.size()\n",
    "        task_embedding = model.thalamus.task_embed(task_id)\n",
    "        task_embedding = model.thalamus.task_proj(task_embedding)\n",
    "        task_embedding = task_embedding.unsqueeze(1).expand(-1, N, -1)\n",
    "        x_combined = story_encoded + task_embedding\n",
    "        x_attn, _ = model.thalamus.scorer(x_combined, question_context, question_context)\n",
    "        scores = x_attn.norm(dim=-1)  # [B, N]\n",
    "        _, topk_indices = scores[0].topk(model.thalamus.k)\n",
    "        topk_indices = topk_indices.cpu().numpy()\n",
    "    \n",
    "    # Convert token indices to words for display\n",
    "    story_words = [idx_to_word[idx.item()] for idx in story_tokens if idx != word_to_idx['<PAD>']]\n",
    "    gated_words = [story_words[idx] if idx < len(story_words) else '<PAD>' for idx in topk_indices]\n",
    "    \n",
    "    # Visualize phase embeddings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot as heatmap\n",
    "    plt.subplot(2, 1, 1)\n",
    "    im = plt.imshow(phase_embeddings, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "    plt.colorbar(im, label='Phase Value')\n",
    "    plt.yticks(range(len(gated_words)), gated_words)\n",
    "    plt.xlabel('Phase Dimension')\n",
    "    plt.title('Phase Embeddings by Token')\n",
    "    \n",
    "    # Plot phase values as sine waves\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, word in enumerate(gated_words[:8]):  # Show only first 8 for clarity\n",
    "        plt.plot(phase_embeddings[i], label=f\"{word}\")\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.xlabel('Phase Dimension')\n",
    "    plt.ylabel('Phase Value')\n",
    "    plt.title('Phase Embeddings as Sine Waves')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample for visualization\n",
    "sample_idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[sample_idx]\n",
    "\n",
    "# Visualize phase embeddings\n",
    "visualize_phase_embeddings(model, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Learning\n",
    "\n",
    "The synthetic thalamus architecture is particularly well-suited for multi-task learning. Let's create a simple experiment with multiple bAbI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_multitask_data(task_ids=[1, 2, 3]):\n",
    "    \"\"\"Prepare a dataset with multiple bAbI tasks.\"\"\"\n",
    "    all_stories, all_questions, all_answers = [], [], []\n",
    "    task_markers = []\n",
    "    \n",
    "    for task_id in task_ids:\n",
    "        stories, questions, answers = parse_babi_task(task_id)\n",
    "        all_stories.extend(stories)\n",
    "        all_questions.extend(questions)\n",
    "        all_answers.extend(answers)\n",
    "        task_markers.extend([task_id-1] * len(stories))  # 0-indexed task IDs\n",
    "    \n",
    "    # Create vocabulary and answer mapping\n",
    "    word_to_idx = create_vocabulary(all_stories, all_questions, all_answers)\n",
    "    vocab_size = len(word_to_idx)\n",
    "    \n",
    "    answer_to_idx = {}\n",
    "    for answer in set(all_answers):\n",
    "        answer_to_idx[answer.lower()] = len(answer_to_idx)\n",
    "    num_answers = len(answer_to_idx)\n",
    "    \n",
    "    print(f\"Total examples: {len(all_stories)}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Number of unique answers: {num_answers}\")\n",
    "    \n",
    "    # Create a dataset class with task IDs\n",
    "    class MultitaskBabiDataset(Dataset):\n",
    "        def __init__(self, stories, questions, answers, task_ids, word_to_idx, answer_to_idx):\n",
    "            self.stories = stories\n",
    "            self.questions = questions\n",
    "            self.answers = answers\n",
    "            self.task_ids = task_ids\n",
    "            self.word_to_idx = word_to_idx\n",
    "            self.answer_to_idx = answer_to_idx\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.stories)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            story = tokenize(self.stories[idx], self.word_to_idx, max_len=100)\n",
    "            question = tokenize(self.questions[idx], self.word_to_idx, max_len=20)\n",
    "            answer = self.answer_to_idx[self.answers[idx].lower()]\n",
    "            task_id = self.task_ids[idx]\n",
    "            \n",
    "            return {\n",
    "                'story': torch.tensor(story, dtype=torch.long),\n",
    "                'question': torch.tensor(question, dtype=torch.long),\n",
    "                'answer': torch.tensor(answer, dtype=torch.long),\n",
    "                'task_id': torch.tensor(task_id, dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = MultitaskBabiDataset(all_stories, all_questions, all_answers, task_markers,\n",
    "                                  word_to_idx, answer_to_idx)\n",
    "    \n",
    "    return dataset, word_to_idx, answer_to_idx\n",
    "\n",
    "# Prepare multitask data\n",
    "multitask_dataset, mt_word_to_idx, mt_answer_to_idx = prepare_multitask_data(task_ids=[1, 2, 3])\n",
    "\n",
    "# Create train/val split\n",
    "train_size = int(0.8 * len(multitask_dataset))\n",
    "val_size = len(multitask_dataset) - train_size\n",
    "mt_train_dataset, mt_val_dataset = torch.utils.data.random_split(multitask_dataset, [train_size, val_size])\n",
    "\n",
    "mt_train_loader = DataLoader(mt_train_dataset, batch_size=32, shuffle=True)\n",
    "mt_val_loader = DataLoader(mt_val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize a new model for multitask learning\n",
    "multitask_model = ThalamusQAModel(vocab_size=len(mt_word_to_idx), num_answers=len(mt_answer_to_idx))\n",
    "\n",
    "# Modify training_step to use the provided task_id\n",
    "def training_step(self, batch, batch_idx):\n",
    "    story = batch['story']\n",
    "    question = batch['question']\n",
    "    answer = batch['answer']\n",
    "    task_id = batch['task_id']\n",
    "    \n",
    "    logits = self(story, question, task_id)\n",
    "    loss = self.loss_fn(logits, answer)\n",
    "    \n",
    "    # Log accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = (preds == answer).float().mean()\n",
    "    \n",
    "    self.log('train_loss', loss)\n",
    "    self.log('train_acc', acc)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    story = batch['story']\n",
    "    question = batch['question']\n",
    "    answer = batch['answer']\n",
    "    task_id = batch['task_id']\n",
    "    \n",
    "    logits = self(story, question, task_id)\n",
    "    loss = self.loss_fn(logits, answer)\n",
    "    \n",
    "    # Log accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = (preds == answer).float().mean()\n",
    "    \n",
    "    self.log('val_loss', loss)\n",
    "    self.log('val_acc', acc)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Replace the training and validation step methods\n",
    "multitask_model.training_step = training_step.__get__(multitask_model)\n",
    "multitask_model.validation_step = validation_step.__get__(multitask_model)\n",
    "\n",
    "# Initialize trainer\n",
    "mt_trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(monitor='val_acc', mode='max'),\n",
    "        pl.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Multitask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the multitask model\n",
    "mt_trainer.fit(multitask_model, mt_train_loader, mt_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Task-Specific Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_per_task(model, dataset):\n",
    "    \"\"\"Evaluate the model's performance on each task separately.\"\"\"\n",
    "    task_correct = {0: 0, 1: 0, 2: 0}\n",
    "    task_total = {0: 0, 1: 0, 2: 0}\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            story = batch['story']\n",
    "            question = batch['question']\n",
    "            answer = batch['answer']\n",
    "            task_id = batch['task_id']\n",
    "            \n",
    "            logits = model(story, question, task_id)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Update counters per task\n",
    "            for i in range(len(task_id)):\n",
    "                t_id = task_id[i].item()\n",
    "                task_total[t_id] += 1\n",
    "                if preds[i] == answer[i]:\n",
    "                    task_correct[t_id] += 1\n",
    "    \n",
    "    # Calculate and print accuracies\n",
    "    print(\"Task-specific performance:\")\n",
    "    accuracies = {}\n",
    "    for task_id in task_correct.keys():\n",
    "        if task_total[task_id] > 0:\n",
    "            acc = task_correct[task_id] / task_total[task_id]\n",
    "            accuracies[task_id] = acc\n",
    "            print(f\"Task {task_id+1}: Accuracy = {acc:.4f} ({task_correct[task_id]}/{task_total[task_id]})\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    task_ids = list(accuracies.keys())\n",
    "    accs = [accuracies[t] for t in task_ids]\n",
    "    \n",
    "    plt.bar([t+1 for t in task_ids], accs)\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance by Task')\n",
    "    plt.xticks([t+1 for t in task_ids])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the multitask model\n",
    "evaluate_per_task(multitask_model, mt_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how the Synthetic Thalamus can be used for question-answering tasks, particularly using the bAbI dataset. The key insights include:\n",
    "\n",
    "1. **Task-Conditioned Processing**: The thalamus can adapt its gating behavior based on the task ID, allowing the model to share parameters while maintaining task-specific processing.\n",
    "\n",
    "2. **Selective Attention**: The thalamus focuses computational resources on the most relevant parts of the input story, similar to how human attention works.\n",
    "\n",
    "3. **Phase Embeddings**: The phase tags add an extra dimension to token representation that could enable more complex context-dependent processing.\n",
    "\n",
    "4. **Multi-Task Learning**: The thalamus architecture naturally supports multi-task learning, allowing the model to learn shared representations while maintaining task-specific behaviors.\n",
    "\n",
    "These properties make the Synthetic Thalamus a promising approach for building more efficient and adaptable neural network architectures for language understanding and reasoning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}